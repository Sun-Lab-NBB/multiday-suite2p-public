from subprocess import run
from multiday_suite2p.cluster.extract import extract_traces_session
from multiday_suite2p.cluster.io  import test_extract_result_present
from pathlib import Path
from suite2p.io.server import ssh_connect
import numpy as np
import uuid 
import re

def extract_job(data_info, settings,data_path, force_recalc=False):
    if (not test_extract_result_present(Path(data_info['data']['local_processed_root'])/data_info['data']['output_folder']/'sessions'/data_path)) | force_recalc:
        multiday_linux = (Path(data_info['data']['server_processed_root'])/data_info['data']['output_folder']).as_posix()
        data_folder = data_info['data']['server_processed_root']
        bin_folder = data_info['data']['server_bin_root']
        data_path = Path(data_path)
        server = settings['server']
        # load info.
        date = data_path.parts[-2]
        session = data_path.parts[-1]
        # cluster parameters.
        job_id = f'{date}-{session}-{uuid.uuid4().hex[:3].upper()}'
        n_cores = server['n_cores']
        # connect to ssh
        ssh = ssh_connect(server['host'], server['username'], server['password'],verbose=False)
        run_command =f'bsub -n {n_cores} -J {job_id} -R"select[avx512]" -o logs/extract-{job_id}.txt "~/extract_session_job.sh'
        # arguments.
        run_command+= f' \'{multiday_linux}\''
        run_command+= f' \'{data_folder}\''
        run_command+= f' \'{bin_folder}\''
        run_command+= f' \'{data_path.as_posix()}\''
        # log location
        run_command+= f' > logs/log-extract-{job_id}.txt"'
        stdin, stdout, stderr = ssh.exec_command(run_command)
        # find job id.
        stdout = stdout.read().decode('utf-8')
        stdout = re.search('Job <(\d*)>',stdout)
        if stdout:
            job_id = int(stdout.group(1))
            print(f'{data_path} - job: {job_id}')
            return {'data_path':data_path,'job_id':job_id}
        else:
            raise NameError("Could not find job id (was job submit succesfull?)")
    else:
        print(f'{data_path} - Already present')
    return

def check_job_status(job,data_info, settings):
    """Checks on status of job generated by send_session_job.

    Args:
        job (dictionary): Must contain:
                        'job_id': Job id on cluster.
                        'data_path': data_path of job (animal/date/session_id)
        server (dictionary): Necessary to connect to cluster.
                        'host'
                        'username'
                        'password'
    """
    server = settings['server']
    ssh = ssh_connect(server['host'], server['username'], server['password'],verbose=False)
    # check job status
    stdin, stdout, stderr = ssh.exec_command(f'bjobs -l { job["job_id"] }')
    status = re.search("Status <.*>",stdout.read().decode('utf-8')).group()
    # check combined folder 
    result_folder = Path(data_info['data']['local_processed_root'])/data_info['data']['output_folder']/'sessions'/job["data_path"]
    if test_extract_result_present(result_folder):
        result_folder = 'present'
    else:
        result_folder = 'absent'
    print(f'{job["data_path"]}: {status}, trace files: {result_folder}')